{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3832dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "os.environ[\"KMP_AFFINITY\"] = \"noverbose\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.autograph.set_verbosity(3)\n",
    "\n",
    "from tensorflow import keras\n",
    "from tqdm.autonotebook import tqdm, trange\n",
    "import scipy as sp\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dropout, Input\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.random import set_seed\n",
    "\n",
    "from spektral.data.loaders import SingleLoader\n",
    "from spektral.datasets.citation import Citation\n",
    "from spektral.layers import GATConv,GCNConv\n",
    "from spektral.transforms import LayerPreprocess\n",
    "from spektral.data.graph import Graph\n",
    "from spektral.data.dataset import Dataset\n",
    "from spektral.data import BatchLoader\n",
    "from spektral.utils import gcn_filter\n",
    "\n",
    "\n",
    "import scipy.sparse as spp\n",
    "from spektral.transforms.normalize_adj import NormalizeAdj\n",
    "from spektral.data import DisjointLoader\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "import polyscope as ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6f31c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Graph-Based Machine Learning Model\n",
    "Now we will train a graph neural network model to perform mesh segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16f90d8-9520-4b36-aaf7-ceda6af42f8a",
   "metadata": {},
   "source": [
    "### Redefine Augmentation Pipeline\n",
    "\n",
    "Just a copy of the previous augmentation function which will enable active augmentation during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1a3818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Augmentation_pipeline(v_mat,ffd_scale = 0.35, scale=0.15, rotation=np.pi/4,translation=0.0):\n",
    "    \n",
    "    ffd = pygem.FFD([4, 4, 4])\n",
    "    ffd.array_mu_x = np.random.uniform(low = -ffd_scale, high=ffd_scale , size=ffd.array_mu_x.shape)\n",
    "    ffd.array_mu_y = np.random.uniform(low = -ffd_scale, high=ffd_scale , size=ffd.array_mu_y.shape)\n",
    "    ffd.array_mu_z = np.random.uniform(low = -ffd_scale, high=ffd_scale , size=ffd.array_mu_z.shape)\n",
    "\n",
    "    v_mat = ffd(v_mat)\n",
    "    \n",
    "    v_mat = (R.from_rotvec(np.random.uniform(low=-rotation,high=rotation,size=3)).as_matrix() @ v_mat.T).T\n",
    "    \n",
    "    v_mat *= (1-np.random.uniform(low=-scale,high=scale))\n",
    "    \n",
    "    v_mat = v_mat - v_mat.min(0)\n",
    "    v_mat = v_mat + (1 - v_mat.max(0))/2\n",
    "    \n",
    "    v_mat += np.random.uniform(low=-translation,high=translation,size=3)\n",
    "    \n",
    "    return v_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754d2c00-b793-4fe4-aae6-325c9ad2785a",
   "metadata": {},
   "source": [
    "### Defining A Graph Dataset\n",
    "\n",
    "Here we have some functions and methods to load the dataset from the numpy file and split it into train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00055594",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aircraft_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, path = 'dataset.npy', subset=\"train\", **kwargs):\n",
    "        \n",
    "        self.raw = np.load(path,allow_pickle=True)\n",
    "        if subset == \"train\":\n",
    "            self.n_samples = int(self.raw.shape[0] * 0.8)\n",
    "            self.st = 0\n",
    "        else:\n",
    "            self.n_samples = self.raw.shape[0]\n",
    "            self.st = int(self.raw.shape[0] * 0.8)\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def read(self):\n",
    "        def make_graph(i):\n",
    "            # Node features\n",
    "            x = self.raw[i,0]\n",
    "            # Edges\n",
    "            a = spp.csr_matrix((np.ones(self.raw[i,-2].shape[0]),(self.raw[i,-2][:,0],self.raw[i,-2][:,1])),shape=[x.shape[0],x.shape[0]])\n",
    "            \n",
    "            return Graph(x=x, a=a, y=i)\n",
    "        graphs = []\n",
    "        for i in range(self.st, self.n_samples):\n",
    "            g = make_graph(i)\n",
    "            if not g is None:\n",
    "                graphs.append(g)\n",
    "        # We must return a list of Graph objects\n",
    "        return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02fe514",
   "metadata": {},
   "outputs": [],
   "source": [
    "gat_data = Aircraft_Dataset(transforms=[LayerPreprocess(GATConv)])\n",
    "gat_data_val = Aircraft_Dataset(transforms=[LayerPreprocess(GATConv)], subset = \"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d39f100-481d-44a3-9f88-3dae6874936e",
   "metadata": {},
   "source": [
    "### Visualize Active Augmentation\n",
    "Here is a quick visualization of the active augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83696ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "mesh = gat_data[5].x\n",
    "new_mesh = Augmentation_pipeline(mesh)\n",
    "\n",
    "ax = plt.figure(figsize=(8,8)).add_subplot(111, projection='3d')\n",
    "ax.scatter(*new_mesh.T,s=1.5)\n",
    "ax.scatter(*mesh.T,s=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1797b2-038e-4173-9d49-159295efe146",
   "metadata": {},
   "source": [
    "### Defining The Model\n",
    "\n",
    "Here we have some model architectures for mesh prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05e5359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 Layer graph attention model applies to node level information in the graph\n",
    "class node_level(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, dropout=0.1, l2_reg = 2.5e-4 , **kwargs):\n",
    "        super(node_level, self).__init__(**kwargs)\n",
    "\n",
    "        self.gc_1 =  GATConv(\n",
    "                        64,\n",
    "                        attn_heads=8,\n",
    "                        concat_heads=True,\n",
    "                        dropout_rate=dropout,\n",
    "                        activation=\"elu\",\n",
    "                        kernel_regularizer=l2(l2_reg),\n",
    "                        attn_kernel_regularizer=l2(l2_reg),\n",
    "                        bias_regularizer=l2(l2_reg))\n",
    "        \n",
    "        self.gc_2 = GATConv(\n",
    "                        64,\n",
    "                        attn_heads=8,\n",
    "                        concat_heads=True,\n",
    "                        dropout_rate=dropout,\n",
    "                        activation=\"elu\",\n",
    "                        kernel_regularizer=l2(l2_reg),\n",
    "                        attn_kernel_regularizer=l2(l2_reg),\n",
    "                        bias_regularizer=l2(l2_reg))\n",
    "        \n",
    "        self.gc_3 = GATConv(\n",
    "                        128,\n",
    "                        attn_heads=8,\n",
    "                        concat_heads=True,\n",
    "                        dropout_rate=dropout,\n",
    "                        activation=\"elu\",\n",
    "                        kernel_regularizer=l2(l2_reg),\n",
    "                        attn_kernel_regularizer=l2(l2_reg),\n",
    "                        bias_regularizer=l2(l2_reg))\n",
    "\n",
    "        self.gc_4 = GATConv(\n",
    "                        128,\n",
    "                        attn_heads=8,\n",
    "                        concat_heads=True,\n",
    "                        dropout_rate=dropout,\n",
    "                        activation=\"elu\",\n",
    "                        kernel_regularizer=l2(l2_reg),\n",
    "                        attn_kernel_regularizer=l2(l2_reg),\n",
    "                        bias_regularizer=l2(l2_reg))\n",
    "        \n",
    "#         self.gc_5 = GATConv(\n",
    "#                         128,\n",
    "#                         attn_heads=8,\n",
    "#                         concat_heads=True,\n",
    "#                         dropout_rate=dropout,\n",
    "#                         activation=\"elu\",\n",
    "#                         kernel_regularizer=l2(l2_reg),\n",
    "#                         attn_kernel_regularizer=l2(l2_reg),\n",
    "#                         bias_regularizer=l2(l2_reg))\n",
    "\n",
    "#         self.gc_6 = GATConv(\n",
    "#                         128,\n",
    "#                         attn_heads=8,\n",
    "#                         concat_heads=True,\n",
    "#                         dropout_rate=dropout,\n",
    "#                         activation=\"elu\",\n",
    "#                         kernel_regularizer=l2(l2_reg),\n",
    "#                         attn_kernel_regularizer=l2(l2_reg),\n",
    "#                         bias_regularizer=l2(l2_reg))\n",
    "        \n",
    "        \n",
    "        self.prd_dense_1 = tf.keras.layers.Dense(256)\n",
    "        self.prd_cbn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.LRelu = tf.keras.layers.LeakyReLU(alpha=0.3)\n",
    "        \n",
    "        self.prd_dense_2 = tf.keras.layers.Dense(256)\n",
    "        self.prd_cbn2 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.prd_dense_3 = tf.keras.layers.Dense(128)\n",
    "        self.prd_cbn3 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.prd_dense_4 = tf.keras.layers.Dense(64)\n",
    "\n",
    "    def call(self, inputs, training = True):\n",
    "        \n",
    "        x,a,_ = inputs\n",
    "        \n",
    "        x = self.gc_1([x,a], training = training)\n",
    "\n",
    "        x = self.gc_2([x,a], training = training)\n",
    "        \n",
    "        x = self.gc_3([x,a], training = training)\n",
    "\n",
    "        x = self.gc_4([x,a], training = training)\n",
    "\n",
    "        # x = self.gc_5([x,a], training = training)\n",
    "\n",
    "        # x = self.gc_6([x,a], training = training)\n",
    "        \n",
    "        x = self.prd_dense_1(x, training = training)\n",
    "        x = self.prd_cbn1(x, training = training)\n",
    "        x = self.LRelu(x)\n",
    "        \n",
    "        x = self.prd_dense_2(x, training = training)\n",
    "        x = self.prd_cbn2(x, training = training)\n",
    "        x = self.LRelu(x)\n",
    "        \n",
    "        x = self.prd_dense_3(x, training = training)\n",
    "        x = self.prd_cbn3(x, training = training)\n",
    "        x = self.LRelu(x)\n",
    "        \n",
    "        x = self.prd_dense_4(x, training = training)\n",
    "        4\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9c672f-1427-45e4-9c0e-6f722f4de437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation Net, details of this are discussed in the PointNet Paper\n",
    "class TransformationNet(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, input_dim = 3):\n",
    "        super(TransformationNet, self).__init__()\n",
    "        self.output_dim = input_dim\n",
    "        \n",
    "        self.conv0 = tf.keras.layers.Conv1D(64,1)\n",
    "        self.ln0 = tf.keras.layers.LayerNormalization()\n",
    "        self.conv1 = tf.keras.layers.Conv1D(128,1)\n",
    "        self.ln1 = tf.keras.layers.LayerNormalization()\n",
    "        self.conv2 = tf.keras.layers.Conv1D(1024,1)\n",
    "        self.ln2 = tf.keras.layers.LayerNormalization()\n",
    "        \n",
    "        self.fc_0  = tf.keras.layers.Dense(512)\n",
    "        self.ln3 = tf.keras.layers.LayerNormalization()\n",
    "        self.fc_1  = tf.keras.layers.Dense(256)\n",
    "        self.ln4 = tf.keras.layers.LayerNormalization()\n",
    "        \n",
    "        self.fc_2  = tf.keras.layers.Dense(input_dim*input_dim)\n",
    "        \n",
    "    def call(self, x):\n",
    "        num_points = x.shape[0]\n",
    "        # x = x.transpose(2, 1)\n",
    "        x = tf.expand_dims(x,0)\n",
    "        x = tf.keras.activations.relu(self.ln0(self.conv0(x)))\n",
    "        x = tf.keras.activations.relu(self.ln1(self.conv1(x)))\n",
    "        x = tf.keras.activations.relu(self.ln2(self.conv2(x)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        x = tf.reduce_max(x,axis=1)\n",
    "        x = tf.reshape(x,[1,1024])\n",
    "\n",
    "        x = tf.keras.activations.relu(self.ln3(self.fc_0(x)))\n",
    "        x = tf.keras.activations.relu(self.ln4(self.fc_1(x)))\n",
    "        x = self.fc_2(x)\n",
    "\n",
    "        identity_matrix = tf.eye(self.output_dim)\n",
    "\n",
    "        x = tf.reshape(x,[self.output_dim,self.output_dim]) + identity_matrix\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eed8b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 layer GAT with global level features extracted\n",
    "class global_level(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, dropout=0.1, l2_reg = 2.5e-4 , **kwargs):\n",
    "        super(global_level, self).__init__(**kwargs)\n",
    "\n",
    "        self.gc_1 =  GATConv(\n",
    "                        64,\n",
    "                        attn_heads=8,\n",
    "                        concat_heads=True,\n",
    "                        dropout_rate=dropout,\n",
    "                        activation=\"elu\",\n",
    "                        kernel_regularizer=l2(l2_reg),\n",
    "                        attn_kernel_regularizer=l2(l2_reg),\n",
    "                        bias_regularizer=l2(l2_reg))\n",
    "        self.pool_1 = spektral.layers.SAGPool(0.5)\n",
    "        \n",
    "        self.gc_2 = GATConv(\n",
    "                        128,\n",
    "                        attn_heads=8,\n",
    "                        concat_heads=True,\n",
    "                        dropout_rate=dropout,\n",
    "                        activation=\"elu\",\n",
    "                        kernel_regularizer=l2(l2_reg),\n",
    "                        attn_kernel_regularizer=l2(l2_reg),\n",
    "                        bias_regularizer=l2(l2_reg))\n",
    "        self.pool_2 = spektral.layers.SAGPool(0.25)\n",
    "        \n",
    "        self.gc_3 = GATConv(\n",
    "                        256,\n",
    "                        attn_heads=8,\n",
    "                        concat_heads=True,\n",
    "                        dropout_rate=dropout,\n",
    "                        activation=\"elu\",\n",
    "                        kernel_regularizer=l2(l2_reg),\n",
    "                        attn_kernel_regularizer=l2(l2_reg),\n",
    "                        bias_regularizer=l2(l2_reg))\n",
    "        \n",
    "        self.g_pool = spektral.layers.GlobalAttentionPool(256)\n",
    "        \n",
    "        self.prd_dense_1 = tf.keras.layers.Dense(256)\n",
    "        self.prd_cbn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.LRelu = tf.keras.layers.LeakyReLU(alpha=0.3)\n",
    "        \n",
    "        self.prd_dense_2 = tf.keras.layers.Dense(256)\n",
    "        self.prd_cbn2 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.prd_dense_3 = tf.keras.layers.Dense(128)\n",
    "        self.prd_cbn3 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.prd_dense_4 = tf.keras.layers.Dense(64)\n",
    "\n",
    "    def call(self, inputs, training = True):\n",
    "        \n",
    "        x,a,i = inputs\n",
    "        \n",
    "        x = self.gc_1([x,a], training = training)\n",
    "        x,a,i = self.pool_1([x,a,i], training = training)\n",
    "        \n",
    "        x = self.gc_2([x,a], training = training)\n",
    "        x,a,i = self.pool_2([x,a,i], training = training)\n",
    "        \n",
    "        x = self.gc_3([x,a], training = training)\n",
    "        x = self.g_pool([x,i], training = training)\n",
    "        \n",
    "        x = self.prd_dense_1(x, training = training)\n",
    "        x = self.prd_cbn1(x, training = training)\n",
    "        x = self.LRelu(x)\n",
    "        \n",
    "        x = self.prd_dense_2(x, training = training)\n",
    "        x = self.prd_cbn2(x, training = training)\n",
    "        x = self.LRelu(x)\n",
    "        \n",
    "        x = self.prd_dense_3(x, training = training)\n",
    "        x = self.prd_cbn3(x, training = training)\n",
    "        x = self.LRelu(x)\n",
    "        \n",
    "        x = self.prd_dense_4(x, training = training)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f48491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Model with local and global networks\n",
    "class surface_classifier(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, dropout=0.1, l2_reg = 2.5e-4, active_augmentation = Augmentation_pipeline, budget = 50000, has_global = True, **kwargs):\n",
    "        super(surface_classifier, self).__init__(**kwargs)\n",
    "\n",
    "        self.has_global = has_global\n",
    "        if has_global:\n",
    "            self.g_m = global_level(dropout,l2_reg)\n",
    "        self.n_m = node_level(dropout,l2_reg)\n",
    "\n",
    "        self.inp_emb_1 = tf.keras.layers.Dense(64)\n",
    "        self.inp_bn_1 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.inp_emb_2 = tf.keras.layers.Dense(128)\n",
    "        self.inp_bn_2 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.inp_emb_3 = tf.keras.layers.Dense(128)\n",
    "        self.inp_bn_3 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.prd_dense_1 = tf.keras.layers.Dense(256)\n",
    "        self.prd_cbn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.LRelu = tf.keras.layers.LeakyReLU(alpha=0.3)\n",
    "        \n",
    "        self.prd_dense_2 = tf.keras.layers.Dense(128)\n",
    "        self.prd_cbn2 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.prd_dense_3 = tf.keras.layers.Dense(64)\n",
    "        self.prd_cbn3 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.prd_dense_4 = tf.keras.layers.Dense(4)\n",
    "\n",
    "        self.budget = budget\n",
    "\n",
    "        self.aug_f = active_augmentation\n",
    "        \n",
    "    def call(self, inputs, training = True):\n",
    "        x,a,i,faces,l,inds = inputs\n",
    "        \n",
    "        x = self.inp_emb_1(x, training=training)\n",
    "        x = self.inp_bn_1(x, training=training)\n",
    "        x = self.LRelu(x)\n",
    "\n",
    "        x = self.inp_emb_2(x, training=training)\n",
    "        x = self.inp_bn_2(x, training=training)\n",
    "        x = self.LRelu(x)\n",
    "\n",
    "        x = self.inp_emb_3(x, training=training)\n",
    "        x = self.inp_bn_3(x, training=training)\n",
    "        x = self.LRelu(x)\n",
    "\n",
    "        batch = [x,a,i]\n",
    "        if self.has_global:\n",
    "            g_features = self.g_m(batch, training = training)\n",
    "        n_features = self.n_m(batch, training = training)\n",
    "        \n",
    "        if self.has_global:\n",
    "            gather_g_f = tf.gather_nd(g_features,tf.expand_dims(inds,-1))\n",
    "        gather_n_f = tf.gather_nd(n_features,tf.reshape(faces,[-1,1]))\n",
    "        gather_n_f = tf.reduce_mean(tf.reshape(gather_n_f,[faces.shape[0],3,-1]), 1)\n",
    "\n",
    "        if self.has_global:\n",
    "            x = tf.concat([gather_n_f,gather_g_f],-1)\n",
    "        else:\n",
    "            x = gather_n_f\n",
    "        \n",
    "        x = self.prd_dense_1(x, training = training)\n",
    "        x = self.prd_cbn1(x, training = training)\n",
    "        x = self.LRelu(x)\n",
    "        \n",
    "        x = self.prd_dense_2(x, training = training)\n",
    "        x = self.prd_cbn2(x, training = training)\n",
    "        x = self.LRelu(x)\n",
    "        \n",
    "        x = self.prd_dense_3(x, training = training)\n",
    "        x = self.prd_cbn3(x, training = training)\n",
    "        x = self.LRelu(x)\n",
    "        \n",
    "        x = self.prd_dense_4(x, training = training)\n",
    "        \n",
    "        return tf.math.softmax(x)\n",
    "    \n",
    "    def get_batch(self,dataset,batch, training = True):\n",
    "        \n",
    "        faces = None\n",
    "        base = 0\n",
    "        for i,mesh in enumerate(dataset.raw[batch[1]]):\n",
    "            if faces is None:\n",
    "                faces = mesh[0,1]\n",
    "                labels = mesh[0,-1]\n",
    "                inds = np.zeros(shape=[faces.shape[0]]) + i\n",
    "                base += mesh[0,0].shape[0]\n",
    "            else:\n",
    "                faces = np.concatenate([faces,mesh[0,1]+base],0)\n",
    "                labels = np.concatenate([labels,mesh[0,-1]],0)\n",
    "                inds = np.concatenate([inds,np.zeros(shape=[mesh[0,1].shape[0]]) + i],0)\n",
    "                base += mesh[0,0].shape[0]\n",
    "\n",
    "        if faces.shape[0] > self.budget:\n",
    "            sub_ind = np.random.choice(faces.shape[0],size=self.budget, replace=False)\n",
    "            faces = faces[sub_ind]\n",
    "            labels = labels[sub_ind]\n",
    "            inds = inds[sub_ind]\n",
    "        \n",
    "        if training:\n",
    "            return [Augmentation_pipeline(batch[0][0]),batch[0][1],batch[0][2],faces,labels.astype(np.float32),inds.astype(np.int32)]\n",
    "        else:\n",
    "            return [batch[0][0],batch[0][1],batch[0][2],faces,labels.astype(np.float32),inds.astype(np.int32)]\n",
    "            \n",
    "    \n",
    "    def evaluate(self, batch):\n",
    "        cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "        m = tf.keras.metrics.CategoricalAccuracy()\n",
    "        m.reset_state()\n",
    "        \n",
    "        y_pred = self.call(batch)\n",
    "\n",
    "        y = batch[4]\n",
    "\n",
    "        loss = cce(y,y_pred)\n",
    "\n",
    "        m.update_state(y,y_pred)\n",
    "        \n",
    "        return loss, m.result().numpy()\n",
    "    \n",
    "    def get_training_step(self):\n",
    " \n",
    "        def train_step(batch,optimizer,m):\n",
    "            cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "            m.reset_state()\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred = self.call(batch)\n",
    "                \n",
    "                y = batch[4]\n",
    "                \n",
    "                loss = cce(y,y_pred)\n",
    "                \n",
    "            gradients = tape.gradient(loss, self.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "            \n",
    "            m.update_state(y,y_pred)\n",
    "            \n",
    "            \n",
    "            return loss, m.result()\n",
    "        \n",
    "        return train_step\n",
    "    \n",
    "    def train(self, dataset, val_data, epochs = 10, lr = 1e-4, batch_size = 2):\n",
    "        \n",
    "        loader = DisjointLoader(dataset, batch_size=batch_size)\n",
    "        loader_val = DisjointLoader(val_data, batch_size=batch_size*3)\n",
    "        n_st = loader.steps_per_epoch\n",
    "        n_st_val = loader_val.steps_per_epoch\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.Adam(lr)\n",
    "        \n",
    "        train_step = self.get_training_step()\n",
    "        m = tf.keras.metrics.CategoricalAccuracy()\n",
    "        for epoch in range(epochs):\n",
    "            prog = trange(n_st)\n",
    "            l_ov = 0.\n",
    "            acc_ov = 0.\n",
    "            n_t = 0\n",
    "            for i in prog:\n",
    "                loader_batch = loader.__next__()\n",
    "\n",
    "                batch = self.get_batch(dataset,loader_batch)\n",
    "                \n",
    "                loss,acc = train_step(batch,optimizer,m)\n",
    "\n",
    "                nf = batch[3].shape[0]\n",
    "                n_t += nf\n",
    "\n",
    "                l_ov += loss * nf\n",
    "                acc_ov += acc * nf\n",
    "\n",
    "                prog.set_postfix_str('Loss: %f, Ovrall Loss: %f, Accuracy: %f, Overall Accuracy: %f' % (loss,l_ov/n_t,acc,acc_ov/n_t))\n",
    "                \n",
    "            print('Epoch %i Loss: %f, Accuracy: %f' % (epoch+1, l_ov/n_t,acc_ov/n_t))\n",
    "            \n",
    "            # prog = trange(n_st_val)\n",
    "            l_ov = 0.\n",
    "            acc_ov = 0.\n",
    "            n_t = 0\n",
    "            for i in range(n_st_val):\n",
    "                loader_batch = loader_val.__next__()\n",
    "\n",
    "                batch = self.get_batch(dataset,loader_batch,training = False)\n",
    "                \n",
    "                loss,acc = self.evaluate(batch)\n",
    "\n",
    "                nf = batch[3].shape[0]\n",
    "                n_t += nf\n",
    "                \n",
    "                l_ov += loss * nf\n",
    "                acc_ov += acc * nf\n",
    "\n",
    "                # prog.set_postfix_str('Loss: %f, Ovrall Loss: %f, Accuracy: %f, Overall Accuracy: %f' % (loss,l_ov/n_t,acc,acc_ov/n_t))\n",
    "                \n",
    "            print('Epoch %i Validatio Loss: %f, Validation Accuracy: %f' % (epoch+1, l_ov/n_t,acc_ov/n_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5a555d-8ec3-4e9f-a18e-92b9dd939ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current model with local and transformation net\n",
    "class surface_classifier_with_tnet(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, dropout=0.1, l2_reg = 2.5e-4, active_augmentation = Augmentation_pipeline, budget = 50000, **kwargs):\n",
    "        super(surface_classifier_with_tnet, self).__init__(**kwargs)\n",
    "\n",
    "        self.t_net = TransformationNet()\n",
    "        self.n_m = node_level(dropout,l2_reg)\n",
    "\n",
    "        self.inp_emb_1 = tf.keras.layers.Dense(128)\n",
    "        self.inp_bn_1 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.inp_emb_2 = tf.keras.layers.Dense(256)\n",
    "        self.inp_bn_2 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.inp_emb_3 = tf.keras.layers.Dense(512)\n",
    "        self.inp_bn_3 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.prd_dense_1 = tf.keras.layers.Dense(512)\n",
    "        self.prd_cbn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.LRelu = tf.keras.layers.LeakyReLU(alpha=0.3)\n",
    "        \n",
    "        self.prd_dense_2 = tf.keras.layers.Dense(1024)\n",
    "        self.prd_cbn2 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.prd_dense_3 = tf.keras.layers.Dense(1024)\n",
    "        self.prd_cbn3 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.prd_dense_4 = tf.keras.layers.Dense(4)\n",
    "\n",
    "        self.budget = budget\n",
    "\n",
    "        self.aug_f = active_augmentation\n",
    "        \n",
    "    def call(self, inputs, training = True):\n",
    "        x,a,i,faces,l,inds = inputs\n",
    "        \n",
    "        T = self.t_net(x)\n",
    "        x = tf.transpose(tf.matmul(T,tf.transpose(x)))\n",
    "        \n",
    "        x = self.inp_emb_1(x, training=training)\n",
    "        x = self.inp_bn_1(x, training=training)\n",
    "        x = self.LRelu(x)\n",
    "\n",
    "        x = self.inp_emb_2(x, training=training)\n",
    "        x = self.inp_bn_2(x, training=training)\n",
    "        x = self.LRelu(x)\n",
    "\n",
    "        x = self.inp_emb_3(x, training=training)\n",
    "        x = self.inp_bn_3(x, training=training)\n",
    "        x = self.LRelu(x)\n",
    "\n",
    "        batch = [x,a,i]\n",
    "        n_features = self.n_m(batch, training = training)\n",
    "        \n",
    "        \n",
    "        gather_n_f = tf.gather_nd(n_features,tf.reshape(faces,[-1,1]))\n",
    "        gather_n_f = tf.reduce_mean(tf.reshape(gather_n_f,[tf.shape(faces)[0],3,-1]), 1)\n",
    "        \n",
    "        x = gather_n_f\n",
    "        \n",
    "        x = self.prd_dense_1(x, training = training)\n",
    "        x = self.prd_cbn1(x, training = training)\n",
    "        x = self.LRelu(x)\n",
    "        \n",
    "        x = self.prd_dense_2(x, training = training)\n",
    "        x = self.prd_cbn2(x, training = training)\n",
    "        x = self.LRelu(x)\n",
    "        \n",
    "        x = self.prd_dense_3(x, training = training)\n",
    "        x = self.prd_cbn3(x, training = training)\n",
    "        x = self.LRelu(x)\n",
    "        \n",
    "        x = self.prd_dense_4(x, training = training)\n",
    "        \n",
    "        return tf.math.softmax(x)\n",
    "    \n",
    "    def get_batch(self, dataset, batch, training = True):\n",
    "        \n",
    "        faces = None\n",
    "        base = 0\n",
    "        for i,mesh in enumerate(dataset.raw[batch[1]]):\n",
    "            if faces is None:\n",
    "                faces = mesh[0,1]\n",
    "                labels = mesh[0,-1]\n",
    "                inds = np.zeros(shape=[faces.shape[0]]) + i\n",
    "                base += mesh[0,0].shape[0]\n",
    "            else:\n",
    "                faces = np.concatenate([faces,mesh[0,1]+base],0)\n",
    "                labels = np.concatenate([labels,mesh[0,-1]],0)\n",
    "                inds = np.concatenate([inds,np.zeros(shape=[mesh[0,1].shape[0]]) + i],0)\n",
    "                base += mesh[0,0].shape[0]\n",
    "\n",
    "        if faces.shape[0] > self.budget:\n",
    "            sub_ind = np.random.choice(faces.shape[0],size=self.budget, replace=False)\n",
    "            faces = faces[sub_ind]\n",
    "            labels = labels[sub_ind]\n",
    "            inds = inds[sub_ind]\n",
    "        \n",
    "        if training:\n",
    "            return [Augmentation_pipeline(batch[0][0]).astype(np.float32),batch[0][1],batch[0][2],faces,labels.astype(np.float32),inds.astype(np.int32)]\n",
    "        else:\n",
    "            return [batch[0][0].astype(np.float32),batch[0][1],batch[0][2],faces,labels.astype(np.float32),inds.astype(np.int32)]\n",
    "            \n",
    "    def evaluate(self, batch):\n",
    "        cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "        m = tf.keras.metrics.CategoricalAccuracy()\n",
    "        m.reset_state()\n",
    "        \n",
    "        y_pred = self.call(batch)\n",
    "\n",
    "        y = batch[4]\n",
    "\n",
    "        loss = cce(y,y_pred)\n",
    "\n",
    "        m.update_state(y,y_pred)\n",
    "        \n",
    "        return loss, m.result()\n",
    "    \n",
    "    def get_training_step(self):\n",
    "        @tf.function(reduce_retracing=True)\n",
    "        def train_step(batch,optimizer,m):\n",
    "            cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "            m.reset_state()\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred = self.call(batch)\n",
    "                \n",
    "                y = batch[4]\n",
    "                \n",
    "                loss = cce(y,y_pred)\n",
    "                \n",
    "            gradients = tape.gradient(loss, self.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "            \n",
    "            m.update_state(y,y_pred)\n",
    "            \n",
    "            \n",
    "            return loss, m.result()\n",
    "        \n",
    "        return train_step\n",
    "    \n",
    "    def evaluate_on_data(self, val_data):\n",
    "        loader_val = DisjointLoader(val_data, batch_size=1)\n",
    "        n_st_val = loader_val.steps_per_epoch\n",
    "        l_ov = 0.\n",
    "        acc_ov = 0.\n",
    "        n_t = 0\n",
    "        for i in range(n_st_val):\n",
    "            loader_batch = loader_val.__next__()\n",
    "\n",
    "            batch = self.get_batch(val_data,loader_batch,training = False)\n",
    "\n",
    "            loss,acc = self.evaluate(batch)\n",
    "\n",
    "            nf = batch[3].shape[0]\n",
    "            n_t += nf\n",
    "\n",
    "            l_ov += loss * nf\n",
    "            acc_ov += acc * nf\n",
    "\n",
    "        print('Validatio Loss: %f, Validation Accuracy: %f' % ( l_ov/n_t,acc_ov/n_t))\n",
    "        \n",
    "        return l_ov/n_t,acc_ov/n_t\n",
    "    \n",
    "    def train(self, dataset, val_data, epochs = 150, lr = 1e-4, save_name=None):\n",
    "        \n",
    "        loader = DisjointLoader(dataset, batch_size=1)\n",
    "        loader_val = DisjointLoader(val_data, batch_size=1)\n",
    "        n_st = loader.steps_per_epoch\n",
    "        n_st_val = loader_val.steps_per_epoch\n",
    "        \n",
    "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(lr,\n",
    "                                                                     decay_steps= n_st*(epochs//15),\n",
    "                                                                     decay_rate=0.65,\n",
    "                                                                     staircase=True)\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        \n",
    "        train_step = self.get_training_step()\n",
    "        m = tf.keras.metrics.CategoricalAccuracy()\n",
    "        \n",
    "        best_val = 0.0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            prog = trange(n_st)\n",
    "            l_ov = 0.\n",
    "            acc_ov = 0.\n",
    "            n_t = 0\n",
    "            for i in prog:\n",
    "                loader_batch = loader.__next__()\n",
    "\n",
    "                batch = self.get_batch(dataset,loader_batch)\n",
    "                loss,acc = train_step(batch,optimizer,m)\n",
    "\n",
    "                nf = batch[3].shape[0]\n",
    "                n_t += nf\n",
    "\n",
    "                l_ov += loss * nf\n",
    "                acc_ov += acc * nf\n",
    "\n",
    "                prog.set_postfix_str('Loss: %f, Ovrall Loss: %f, Accuracy: %f, Overall Accuracy: %f, Learning Rate: %e' % (loss,l_ov/n_t,acc,acc_ov/n_t,optimizer._decayed_lr(tf.float32)))\n",
    "                \n",
    "            print('Epoch %i Loss: %f, Accuracy: %f' % (epoch+1, l_ov/n_t,acc_ov/n_t))\n",
    "            \n",
    "            # prog = trange(n_st_val)\n",
    "            l_ov = 0.\n",
    "            acc_ov = 0.\n",
    "            n_t = 0\n",
    "            for i in range(n_st_val):\n",
    "                loader_batch = loader_val.__next__()\n",
    "\n",
    "                batch = self.get_batch(dataset,loader_batch,training = False)\n",
    "                \n",
    "                loss,acc = self.evaluate(batch)\n",
    "\n",
    "                nf = batch[3].shape[0]\n",
    "                n_t += nf\n",
    "                \n",
    "                l_ov += loss * nf\n",
    "                acc_ov += acc * nf\n",
    "\n",
    "                # prog.set_postfix_str('Loss: %f, Ovrall Loss: %f, Accuracy: %f, Overall Accuracy: %f' % (loss,l_ov/n_t,acc,acc_ov/n_t))\n",
    "                \n",
    "            print('Epoch %i Validatio Loss: %f, Validation Accuracy: %f' % (epoch+1, l_ov/n_t,acc_ov/n_t))\n",
    "            \n",
    "            if acc_ov/n_t>=best_val:\n",
    "                best_val = acc_ov/n_t\n",
    "                if not save_name is None:\n",
    "                    self.save_weights(save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d594e3-1aef-43d2-933e-86329b76f175",
   "metadata": {},
   "source": [
    "### Training The Model On the Data\n",
    "Now we will train the model. Note that best performinh checkpoints are saved in the same folder with save_name indicated in the training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2399c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_models = surface_classifier_with_tnet(budget=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de9c935",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training and save best model in the CheckPoints Folder takes about 4 hours to train (uncomment the line below to train)\n",
    "# test_models.train(gat_data,gat_data_val,lr=1e-4, epochs=150,save_name=\"./CheckPoints/T-Net_Classifier\")\n",
    "\n",
    "# Or load a pre-trained checkpoint here if you do not want to spend the time training\n",
    "test_models.load_weights('./CheckPoints/T-Net_Classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b1f58c-ddd8-4e61-afb3-3ec1b8d0e434",
   "metadata": {},
   "source": [
    "### Evaluate The Trained Model\n",
    "Below is a few lines of code to evaluate and visulize the model.\n",
    "\n",
    "#### Loss And Accuracy on Validation Data\n",
    "We will measure the accuracy of the model on data it has not seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e078fa34-babe-48d9-8c5f-70ea4bd72347",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss,acc = test_models.evaluate_on_data(gat_data_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e127e4e1-9844-4cbc-ab19-5291ae2ac499",
   "metadata": {},
   "source": [
    "#### Visualization Functions\n",
    "\n",
    "Functions thaty allow rendering preditcions on the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468f8343-ced6-4017-b104-29bbc8e2f4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_prediction(v,f,l,save_file ='render.png', view='isometric',shadow=False,colorize=True, def_color=[0.7,0.7,0.7]):\n",
    "    v_mat = v\n",
    "    f_mat = f\n",
    "    label_mat = l\n",
    "\n",
    "    ps.set_autocenter_structures(True)\n",
    "    ps.set_autoscale_structures(True)\n",
    "    ps.init()\n",
    "\n",
    "    if colorize:\n",
    "        ps_mesh = ps.register_surface_mesh(\"my mesh\", v_mat, f_mat[np.where(label_mat[:,0])[0]],color=[1.0,0.0,0.0])\n",
    "        ps_mesh1 = ps.register_surface_mesh(\"my mesh 1\", v_mat, f_mat[np.where(label_mat[:,1])[0]],color=[0.0,1.0,0.0])\n",
    "        ps_mesh2 = ps.register_surface_mesh(\"my mesh 2\", v_mat, f_mat[np.where(label_mat[:,2])[0]],color=[0.0,0.0,1.0])\n",
    "        ps_mesh3 = ps.register_surface_mesh(\"my mesh 3\", v_mat, f_mat[np.where(label_mat[:,3])[0]],color=[0.0,1.0,1.0])\n",
    "    else:\n",
    "        ps_mesh = ps.register_surface_mesh(\"my mesh\", v_mat, f_mat,color=def_color)\n",
    "\n",
    "    ps.set_ground_plane_height_factor(0.08)\n",
    "    ps.set_up_dir(\"z_up\")\n",
    "\n",
    "    if shadow:\n",
    "        ps.set_ground_plane_mode(\"shadow_only\")\n",
    "    else:\n",
    "        ps.set_ground_plane_mode(\"none\")\n",
    "\n",
    "\n",
    "    if view == 'isometric':\n",
    "        ps.look_at((-1.0,-1.0,1.0),(.0, .0, .0))\n",
    "    elif view == 'top':\n",
    "        ps.look_at((-0.0,-0.001,1.8),(.0, .0, .0))\n",
    "    elif view == 'bottom':\n",
    "        ps.look_at((-0.0,-0.001,-1.8),(.0, .0, .0))\n",
    "    elif view == 'left':\n",
    "        ps.look_at((-0.0,-1.8,0.0),(.0, .0, .0))\n",
    "    elif view == 'right':\n",
    "        ps.look_at((-0.0,1.8,0.0),(.0, .0, .0))\n",
    "    elif view == 'front':\n",
    "        ps.look_at((-1.8,0.0,0.0),(.0, .0, .0))\n",
    "    elif view == 'back':\n",
    "        ps.look_at((1.8,0.0,0.0),(.0, .0, .0))\n",
    "\n",
    "    ps.screenshot(filename=save_file)\n",
    "    \n",
    "def render_centainty(v,f,l,l_pred,save_file ='render.png', view='isometric',shadow=False,colorize=True, def_color=[0.7,0.7,0.7]):\n",
    "    v_mat = v\n",
    "    f_mat = f\n",
    "    label_mat = l\n",
    "\n",
    "    ps.set_autocenter_structures(True)\n",
    "    ps.set_autoscale_structures(True)\n",
    "    ps.init()\n",
    "\n",
    "    if colorize:\n",
    "        for i,face in enumerate(f_mat):\n",
    "            ps.register_surface_mesh(\"my mesh %i\"%(i), v_mat, [face],color=[0.0,0.0,(l[i]*l_pred[i]).sum()])\n",
    "\n",
    "    else:\n",
    "        ps_mesh = ps.register_surface_mesh(\"my mesh\", v_mat, f_mat,color=def_color)\n",
    "\n",
    "    ps.set_ground_plane_height_factor(0.08)\n",
    "    ps.set_up_dir(\"z_up\")\n",
    "\n",
    "    if shadow:\n",
    "        ps.set_ground_plane_mode(\"shadow_only\")\n",
    "    else:\n",
    "        ps.set_ground_plane_mode(\"none\")\n",
    "\n",
    "\n",
    "    if view == 'isometric':\n",
    "        ps.look_at((-1.0,-1.0,1.0),(.0, .0, .0))\n",
    "    elif view == 'top':\n",
    "        ps.look_at((-0.0,-0.001,1.8),(.0, .0, .0))\n",
    "    elif view == 'bottom':\n",
    "        ps.look_at((-0.0,-0.001,-1.8),(.0, .0, .0))\n",
    "    elif view == 'left':\n",
    "        ps.look_at((-0.0,-1.8,0.0),(.0, .0, .0))\n",
    "    elif view == 'right':\n",
    "        ps.look_at((-0.0,1.8,0.0),(.0, .0, .0))\n",
    "    elif view == 'front':\n",
    "        ps.look_at((-1.8,0.0,0.0),(.0, .0, .0))\n",
    "    elif view == 'back':\n",
    "        ps.look_at((1.8,0.0,0.0),(.0, .0, .0))\n",
    "\n",
    "    ps.screenshot(filename=save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9aa7b53-46e9-42dc-b264-78f846ea8c6d",
   "metadata": {},
   "source": [
    "#### Other Metrics & And Rendering\n",
    "The predictions are visualized in the visualization folder with pred files being prediction and gt files being ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711ed78c-32ed-4f61-8605-8a9b272c1607",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = tf.keras.metrics.TopKCategoricalAccuracy(k=2)\n",
    "m.reset_state()\n",
    "wrong_certainty = 0.\n",
    "correct_certainty = 0.\n",
    "nc = 0\n",
    "nw = 0\n",
    "loader = DisjointLoader(gat_data, batch_size=1,shuffle=False)\n",
    "for g in tqdm(gat_data_val):\n",
    "    loader_batch = loader.collate([g])\n",
    "    ind = loader_batch[1][0][0]\n",
    "    faces = gat_data_val.raw[ind,1]\n",
    "    labels = gat_data_val.raw[ind,-1]\n",
    "    x = loader_batch[0][0].astype(np.float32)\n",
    "    a = loader_batch[0][1]\n",
    "    i = loader_batch[0][2]\n",
    "    \n",
    "    l_pred = test_models([x,a,i,faces,labels,0])\n",
    "    l_oh = tf.one_hot(tf.argmax(l_pred,axis=-1),4).numpy().astype(bool)\n",
    "    \n",
    "    render_prediction(x,faces,l_oh,\"./visualization/%i_pred.png\"%(ind))\n",
    "    render_prediction(x,faces,labels,\"./visualization/%i_gt.png\"%(ind))\n",
    "    m.update_state(labels,l_pred)\n",
    "    \n",
    "    l_wrong = l_pred.numpy()[(labels * l_oh).sum(-1)==0]\n",
    "    wrong_certainty += l_wrong[list(range(l_wrong.shape[0])),np.argmax(l_wrong,axis=-1)].sum()\n",
    "    \n",
    "    l_correct = l_pred.numpy()[(labels * l_oh).sum(-1)!=0]\n",
    "    correct_certainty += l_correct[list(range(l_correct.shape[0])),np.argmax(l_correct,axis=-1)].sum()\n",
    "    \n",
    "    nc += l_correct.shape[0]\n",
    "    nw += l_wrong.shape[0]\n",
    "    \n",
    "print(\"Top 2 accuracy: %f\"%(m.result().numpy()))\n",
    "print(\"Certainty on Correct Predictions: %f\"%(correct_certainty/nc))\n",
    "print(\"Certainty on Incorrect Predictions: %f\"%(wrong_certainty/nw))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b62af4888e87da53f856de2ffc6c1d67a690e90421b94d202eec01971c9cdc02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
